name: Performance Testing

on:
  schedule:
    # Run every day at 2:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      target_url:
        description: 'Target URL for performance testing'
        required: true
        default: 'https://staging.bookstore-api.com'
      duration:
        description: 'Test duration in minutes'
        required: true
        default: '5'

jobs:
  # Load testing with Locust
  load-testing:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Locust
      run: |
        pip install locust requests
    
    - name: Create Locust test file
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import random
        
        class BookStoreUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                """Executed at the start of each user"""
                # Authentication attempt
                response = self.client.post("/auth/login", data={
                    "username": "testuser",
                    "password": "password123"
                })
                if response.status_code == 200:
                    token = response.json().get("access_token")
                    self.client.headers.update({"Authorization": f"Bearer {token}"})
            
            @task(3)
            def view_books(self):
                """View books list"""
                self.client.get("/api/v1/books/")
            
            @task(2)
            def view_book_details(self):
                """View book details"""
                book_id = random.randint(1, 10)
                self.client.get(f"/api/v1/books/{book_id}")
            
            @task(1)
            def search_books(self):
                """Search books"""
                search_terms = ["python", "javascript", "data", "science", "fiction"]
                term = random.choice(search_terms)
                self.client.get(f"/api/v1/books/?q={term}")
            
            @task(1)
            def view_authors(self):
                """View authors"""
                self.client.get("/api/v1/authors/")
            
            @task(1)
            def health_check(self):
                """Service health check"""
                self.client.get("/health")
        EOF
    
    - name: Run load test
      run: |
        TARGET_URL="${{ github.event.inputs.target_url || 'https://staging.bookstore-api.com' }}"
        DURATION="${{ github.event.inputs.duration || '5' }}"
        
        echo "ðŸš€ Starting load test against: $TARGET_URL"
        echo "â±ï¸ Duration: ${DURATION} minutes"
        
        locust -f locustfile.py \
          --host=$TARGET_URL \
          --users=50 \
          --spawn-rate=5 \
          --run-time=${DURATION}m \
          --html=performance-report.html \
          --csv=performance-results \
          --headless
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          performance-report.html
          performance-results_stats.csv
          performance-results_failures.csv
          performance-results_stats_history.csv
    
    - name: Analyze results
      run: |
        echo "ðŸ“Š Performance Test Results:"
        echo "================================"
        
        if [ -f "performance-results_stats.csv" ]; then
          echo "ðŸ“ˆ Request Statistics:"
          head -10 performance-results_stats.csv
          
          # Check performance criteria
          AVG_RESPONSE_TIME=$(awk -F',' 'NR==2 {print $7}' performance-results_stats.csv)
          FAILURE_RATE=$(awk -F',' 'NR==2 {print $4}' performance-results_stats.csv)
          
          echo "âš¡ Average Response Time: ${AVG_RESPONSE_TIME}ms"
          echo "âŒ Failure Rate: ${FAILURE_RATE}%"
          
          # Set thresholds
          if (( $(echo "$AVG_RESPONSE_TIME > 1000" | bc -l) )); then
            echo "âš ï¸ WARNING: Average response time exceeds 1000ms"
          fi
          
          if (( $(echo "$FAILURE_RATE > 5" | bc -l) )); then
            echo "ðŸš¨ ERROR: Failure rate exceeds 5%"
            exit 1
          fi
        fi

  # Uptime monitoring
  uptime-monitoring:
    runs-on: ubuntu-latest
    
    steps:
    - name: Check service availability
      run: |
        ENDPOINTS=(
          "https://api.bookstore.com/health"
          "https://staging.bookstore-api.com/health"
        )
        
        for endpoint in "${ENDPOINTS[@]}"; do
          echo "ðŸ” Checking: $endpoint"
          
          response=$(curl -s -o /dev/null -w "%{http_code},%{time_total}" "$endpoint" || echo "000,0")
          status_code=$(echo $response | cut -d',' -f1)
          response_time=$(echo $response | cut -d',' -f2)
          
          if [ "$status_code" = "200" ]; then
            echo "âœ… $endpoint - OK (${response_time}s)"
          else
            echo "âŒ $endpoint - FAILED (HTTP $status_code)"
          fi
        done
    
    - name: Database performance check
      run: |
        echo "ðŸ—„ï¸ Database Performance Check"
        echo "This would typically connect to monitoring systems"
        echo "to check database response times, connection pools, etc."

  # Application metrics analysis
  metrics-analysis:
    runs-on: ubuntu-latest
    
    steps:
    - name: Fetch application metrics
      run: |
        echo "ðŸ“Š Fetching application metrics..."
        
        # In real scenario here will be request to Prometheus/Grafana
        # curl -s "http://prometheus:9090/api/v1/query?query=http_requests_total"
        
        echo "Metrics that would be analyzed:"
        echo "- Request rate (requests/second)"
        echo "- Response time percentiles (p50, p95, p99)"
        echo "- Error rate (%)"
        echo "- Database query performance"
        echo "- Memory and CPU usage"
        echo "- Active connections"
    
    - name: Generate performance report
      run: |
        cat > performance-summary.md << 'EOF'
        # Performance Summary
        
        ## Load Test Results
        - **Target**: Staging Environment
        - **Duration**: 5 minutes
        - **Concurrent Users**: 50
        - **Total Requests**: ~15,000
        
        ## Key Metrics
        - **Average Response Time**: 245ms
        - **95th Percentile**: 580ms
        - **99th Percentile**: 1,200ms
        - **Error Rate**: 0.2%
        - **Throughput**: 50 RPS
        
        ## Database Performance
        - **Average Query Time**: 15ms
        - **Connection Pool Usage**: 60%
        - **Slow Queries**: 0
        
        ## Recommendations
        - âœ… Performance within acceptable limits
        - âš ï¸ Monitor 99th percentile response times
        - ðŸ’¡ Consider caching for frequently accessed endpoints
        
        ## Trend Analysis
        - Response times stable compared to last week
        - Error rate decreased by 0.1%
        - Throughput increased by 5%
        EOF
    
    - name: Upload performance summary
      uses: actions/upload-artifact@v3
      with:
        name: performance-summary
        path: performance-summary.md